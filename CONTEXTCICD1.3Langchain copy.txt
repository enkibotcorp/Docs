# ContextCICD1.3: Dockerización del Servicio LangChain y Resolución de Problemas

## Resumen Ejecutivo

Hemos completado exitosamente la dockerización del servicio LangChain, un componente crítico de nuestra plataforma que maneja la lógica conversacional y la integración con modelos de lenguaje como OpenAI GPT y Google Gemini. Este documento detalla el proceso de implementación, los desafíos encontrados, las soluciones aplicadas y las lecciones aprendidas durante el proceso.

## Arquitectura Implementada

La arquitectura de CI/CD para el servicio LangChain se compone de:

1. **Contenedor Docker**:
   - Imagen base: python:3.9-slim
   - Exposición de puertos: 5000 (principal), 5001-5002 (fallback)
   - Configuración de entorno para comunicación con otros servicios
   - Integración con múltiples modelos de lenguaje (OpenAI, Google Gemini)

2. **Docker Compose**:
   - Orquestación del servicio LangChain
   - Configuración de variables de entorno
   - Mapeo de puertos para acceso externo
   - Integración con la red compartida `enkisys_net`

3. **Estructura del Proyecto**:
   - Dockerfile para la definición del contenedor
   - docker-compose.yml para la orquestación
   - requirements.txt para la gestión de dependencias Python

4. **Integración con Otros Servicios**:
   - Comunicación con el servicio de herramientas (Tool Manager)
   - Integración con el módulo RAG para recuperación de información contextual
   - Conexión con servicios de autenticación

## Desafíos y Soluciones

### 1. Error de Dependencias Faltantes

**Desafío**: El servicio LangChain requería la biblioteca `langchain-google-genai` para la integración con Google Gemini, pero esta dependencia no estaba incluida en el archivo requirements.txt.

**Solución**:
- Identificamos el error en los logs del contenedor
- Actualizamos el archivo requirements.txt para incluir la dependencia faltante
- Reconstruimos la imagen Docker con las dependencias actualizadas

```bash
# Comando utilizado para añadir la dependencia
echo "langchain-google-genai" >> requirements.txt

# Reconstrucción de la imagen
docker-compose build
```

### 2. Error KeyError: 'ContainerConfig'

**Desafío**: Durante el despliegue con Docker Compose, encontramos un error persistente relacionado con la configuración del contenedor: `KeyError: 'ContainerConfig'`.

**Solución**:
- Realizamos una limpieza completa de recursos Docker para eliminar cualquier estado corrupto:
```bash
# Detener y eliminar el contenedor
docker stop enkisys_langchain
docker rm enkisys_langchain

# Eliminar la imagen
docker rmi langchain_langchain

# Limpiar recursos no utilizados
docker system prune -a --volumes -f
```

- Utilizamos comandos Docker directos en lugar de Docker Compose para evitar problemas de recreación de contenedores:
```bash
# Crear la red si no existe
docker network create enkisys_net

# Construir la imagen manualmente
docker build -t enkisys_langchain:latest .

# Ejecutar el contenedor manualmente
docker run -d --name enkisys_langchain \
  -p 5000:5000 -p 5001:5001 -p 5002:5002 \
  --network enkisys_net \
  enkisys_langchain:latest
```

### 3. Gestión de Múltiples Modelos de Lenguaje

**Desafío**: El servicio LangChain necesita integrar múltiples modelos de lenguaje (OpenAI y Google Gemini) y seleccionar el adecuado según la solicitud.

**Solución**:
- Implementamos una configuración flexible que permite seleccionar el modelo en tiempo de ejecución
- Configuramos variables de entorno para las claves API de cada proveedor
- Añadimos lógica para manejar fallbacks en caso de que un modelo no esté disponible

## Comandos Útiles para Gestión y Monitoreo

### Gestión de Contenedores

```bash
# Ver contenedores en ejecución
docker ps

# Detener el contenedor LangChain
docker stop enkisys_langchain

# Iniciar el contenedor LangChain
docker start enkisys_langchain

# Reiniciar el contenedor LangChain
docker restart enkisys_langchain
```

### Monitoreo de Logs

```bash
# Ver logs del servicio LangChain
docker logs enkisys_langchain

# Ver logs en tiempo real
docker logs -f enkisys_langchain

# Ver las últimas 100 líneas de logs
docker logs --tail 100 enkisys_langchain

# Ver logs con timestamp
docker logs --timestamps enkisys_langchain
```

### Acceso al Contenedor

```bash
# Acceder al shell del contenedor
docker exec -it enkisys_langchain bash

# Ejecutar un comando específico en el contenedor
docker exec enkisys_langchain python -c "import sys; print(sys.version)"

# Verificar las variables de entorno en el contenedor
docker exec enkisys_langchain env
```

### Verificación de Salud del Servicio

```bash
# Verificar si el servicio está respondiendo (asumiendo un endpoint de salud)
curl http://localhost:5000/health

# Verificar la conectividad desde otro contenedor en la misma red
docker exec ms-auth curl http://enkisys_langchain:5000/health
```

## Lecciones Aprendidas

1. **Importancia de la Gestión de Dependencias**: Es crucial mantener un archivo requirements.txt actualizado con todas las dependencias necesarias, incluyendo las integraciones con servicios externos como Google Gemini.

2. **Problemas con el Estado Interno de Docker**: El error `KeyError: 'ContainerConfig'` está relacionado con el estado interno de Docker y puede ocurrir cuando:
   - Hay metadatos corruptos en imágenes o contenedores
   - Existen problemas de compatibilidad entre versiones de Docker y Docker Compose
   - Se interrumpen operaciones de construcción o despliegue

3. **Estrategias de Limpieza**: La limpieza completa de recursos Docker (contenedores, imágenes, volúmenes) es una estrategia efectiva para resolver problemas persistentes relacionados con el estado interno de Docker.

4. **Alternativas a Docker Compose**: En casos donde Docker Compose presenta problemas, los comandos Docker directos pueden ser una alternativa viable, aunque requieren más configuración manual.

5. **Monitoreo de Logs**: El monitoreo constante de logs es esencial para identificar rápidamente problemas en la inicialización y ejecución de servicios contenerizados.

## Consideraciones para Futuros Despliegues

1. **Gestión de Secretos**: Las claves API para servicios externos (OpenAI, Google) deben gestionarse de forma segura, preferiblemente usando servicios de gestión de secretos o variables de entorno inyectadas durante el despliegue.

2. **Escalabilidad**: El servicio LangChain puede requerir escalado horizontal para manejar cargas elevadas. Considerar la implementación de múltiples instancias detrás de un balanceador de carga.

3. **Monitoreo Avanzado**: Implementar métricas y alertas para monitorear el rendimiento del servicio, incluyendo:
   - Tiempo de respuesta de los modelos de lenguaje
   - Tasa de errores en las solicitudes a APIs externas
   - Uso de memoria y CPU

4. **Gestión de Caché**: Considerar la implementación de mecanismos de caché para respuestas frecuentes, reduciendo la carga en las APIs externas y mejorando el tiempo de respuesta.

5. **Pruebas Automatizadas**: Desarrollar pruebas automatizadas para verificar la integración con diferentes modelos de lenguaje y la comunicación con otros servicios.

## Próximos Pasos

1. **Dockerización del Servicio RAG**: El siguiente servicio a dockerizar es el módulo RAG (Retrieval Augmented Generation), que proporciona información contextual al servicio LangChain.

2. **Integración con API Gateway**: Configurar el API Gateway para enrutar solicitudes al servicio LangChain en diferentes entornos (producción, staging, desarrollo).

3. **Implementación de CI/CD**: Configurar workflows en GitHub Actions para automatizar la construcción, prueba y despliegue del servicio LangChain.

4. **Documentación de API**: Desarrollar documentación detallada de la API del servicio LangChain para facilitar la integración con otros componentes.

5. **Optimización de Rendimiento**: Analizar y optimizar el rendimiento del servicio, especialmente en la comunicación con APIs externas y en el procesamiento de solicitudes complejas.

## Conclusión

La dockerización exitosa del servicio LangChain representa un avance significativo en nuestra estrategia de CI/CD y en la modernización de nuestra infraestructura. A pesar de los desafíos encontrados, hemos logrado implementar una solución robusta que integra múltiples modelos de lenguaje y se comunica eficientemente con otros servicios de nuestra plataforma.

Las lecciones aprendidas durante este proceso serán invaluables para la dockerización de los servicios restantes, especialmente en lo relacionado con la gestión de dependencias, la resolución de problemas de Docker y la integración de servicios externos.
